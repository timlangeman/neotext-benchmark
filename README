<h1>Summary<h1>
I'd like to improve the <b>scalability</b> of a program that scrapes user-submitted page URLs and the citations found therein.

The challenge is to develop a script that <b>handles the io asynchronously</b>:
<ul>
 <li>Lookup of the source page</li>
 <li>Lookup of the cited pages</li>
 <li>Process the cited pages: </li>
 <li>Save to DB</li>
 <li>Upload JSON to Amazon S3</li>
</ul>

What is the <b>optimal way of doing this</b> without making the code hard to read?

I would think that the average request time should be able to be shrunk down to less than 2 seconds if everything is asynchronous.

Currently <b>it takes over 10 seconds</b> and blocks, meaning that a 2-core CPU can do ~ 12 requests per minute.

I've included notes about libraries used in actual project in case you have any ideas about to <b>minimize memory usage</b>.

Also note that I intend to gather user submissions and process them in a queue like Celery and the user does not need to wait for the results.
